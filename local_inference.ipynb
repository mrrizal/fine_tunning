{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9915e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4912603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "model_id = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f089badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model (CPU, float16)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model (CPU, float16)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cpu\",   # CPU-only inference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63c2a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt_generator import CodeReviewPromptGenerator\n",
    "prompt_generator = CodeReviewPromptGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2206aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte avg max token\n",
    "import os\n",
    "import json\n",
    "\n",
    "datasets = os.listdir('./synthetic_datasets')\n",
    "\n",
    "max_tokens_code_review_result = []\n",
    "max_tokens_code_review_prompt = []\n",
    "prompts = []\n",
    "for dataset in datasets:\n",
    "    with open(f'./synthetic_datasets/{dataset}', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for i in data:\n",
    "            max_tokens_code_review_result.append(len(i['code_review_suggestion']))\n",
    "            prompt = prompt_generator.generate_style_review_prompt(\n",
    "                added_code=i['added_code'],\n",
    "                deleted_code=i['deleted_code'],\n",
    "                full_function_code=i['full_function_code'],\n",
    "                function_name=i['function_name'],\n",
    "            )\n",
    "            max_tokens_code_review_prompt.append(len(prompt))\n",
    "            prompts.append(prompt)\n",
    "\n",
    "avg_max_tokens_code_review_result = sum(max_tokens_code_review_result) / len(max_tokens_code_review_result) if max_tokens_code_review_result else 0\n",
    "avg_max_tokens_code_review_prompt = sum(max_tokens_code_review_prompt) / len(max_tokens_code_review_prompt) if max_tokens_code_review_prompt else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac3c4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_max_tokens_code_review_result: 466.73426573426576\n",
      "avg_max_tokens_code_review_prompt: 1271.8592657342658\n"
     ]
    }
   ],
   "source": [
    "print(\"avg_max_tokens_code_review_result:\", avg_max_tokens_code_review_result)\n",
    "print(\"avg_max_tokens_code_review_prompt:\", avg_max_tokens_code_review_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84105254",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = random.choices(prompts, k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acb2add4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt ---\n",
      "\n",
      "You are a code reviewer. Analyze this Python code change and respond EXACTLY in the format below.\n",
      "\n",
      "Full function `optimize_similar_text_search`:\n",
      "```python\n",
      "def optimize_similar_text_search(query, documents):\n",
      "    cache = {}\n",
      "    def cache_results(func):\n",
      "        def wrapper(*args):\n",
      "            if args in cache:\n",
      "                return cache[args]\n",
      "            result = func(*args)\n",
      "            cache[args] = result\n",
      "            return result\n",
      "        return wrapper\n",
      "    @cache_results\n",
      "    def search_document(document):\n",
      "        return document.count(query)\n",
      "    results = [search_document(document) for document in documents]\n",
      "    return results\n",
      "```\n",
      "ADDED:\n",
      "```python\n",
      "def cache_results(func):\n",
      "        def wrapper(*args):\n",
      "            if args in cache:\n",
      "                return cache[args]\n",
      "            result = func(*args)\n",
      "            cache[args] = result\n",
      "            return result\n",
      "```\n",
      "REMOVED:\n",
      "```python\n",
      "results = [document.count(query) for document in documents]\n",
      "```\n",
      "\n",
      "\n",
      "You MUST respond in this EXACT format (copy the headers exactly):\n",
      "\n",
      "SUMMARY: [One sentence describing what changed]\n",
      "\n",
      "ISSUES: [List specific bugs/problems, or write \"None found\"]\n",
      "\n",
      "IMPROVEMENTS: [Suggest specific improvements, or write \"None needed\"]\n",
      "\n",
      "DECISION: [Yes/No] - [One sentence reason]\n",
      "\n",
      "Do not add extra text or explanations outside this format.\n",
      "\n",
      "--- Generated Output ---\n",
      "\n",
      "You are a code reviewer. Analyze this Python code change and respond EXACTLY in the format below.\n",
      "\n",
      "Full function `optimize_similar_text_search`:\n",
      "```python\n",
      "def optimize_similar_text_search(query, documents):\n",
      "    cache = {}\n",
      "    def cache_results(func):\n",
      "        def wrapper(*args):\n",
      "            if args in cache:\n",
      "                return cache[args]\n",
      "            result = func(*args)\n",
      "            cache[args] = result\n",
      "            return result\n",
      "        return wrapper\n",
      "    @cache_results\n",
      "    def search_document(document):\n",
      "        return document.count(query)\n",
      "    results = [search_document(document) for document in documents]\n",
      "    return results\n",
      "```\n",
      "ADDED:\n",
      "```python\n",
      "def cache_results(func):\n",
      "        def wrapper(*args):\n",
      "            if args in cache:\n",
      "                return cache[args]\n",
      "            result = func(*args)\n",
      "            cache[args] = result\n",
      "            return result\n",
      "```\n",
      "REMOVED:\n",
      "```python\n",
      "results = [document.count(query) for document in documents]\n",
      "```\n",
      "\n",
      "\n",
      "You MUST respond in this EXACT format (copy the headers exactly):\n",
      "\n",
      "SUMMARY: [One sentence describing what changed]\n",
      "\n",
      "ISSUES: [List specific bugs/problems, or write \"None found\"]\n",
      "\n",
      "IMPROVEMENTS: [Suggest specific improvements, or write \"None needed\"]\n",
      "\n",
      "DECISION: [Yes/No] - [One sentence reason]\n",
      "\n",
      "Do not add extra text or explanations outside this format. Provide just one answer per point on each section according to your judgment of how you would describe these points from an analysis like any other coding problem that was provided above it without looking at additional context unless otherwise specified by me herein handing over my response as given because I need more information about those parts which might be different based upon situation-case etc.. – This is done using markdown syntax only please note all changes should align with python style guide rule regarding white space so indentation could make things worse i will look into why they were added but currently there's no requirement mentioned explaining them... So again do provide accurate answers considering case sensitivity while answering.\"]. You can use MarkDown language where every line starting word starts new paragraph due strictly adherance towards guidelines set during meeting session ... Please let us know when we have next batch ..\"]: Yes   [-If yes then specify brief description.] Here goes .......]\"]. If removed Then \"[Specify Reason]\" ]](https://www2159876430dcbaa@googleclassroompypihostedcontentcom//unauthorized?export=download).\", ], [\"This solution has several issues\"],[\"The improvement suggestions include caching previously calculated values instead saving repeated computation time\",\"Decision\":\"NO\"];}],,\"reply\":{\"summaryOfChangesDescriptionTextFieldKeyNameExistOrNotBooleanValueExistsAndIsFalse\"}, {\"Issues Description Text Field Key Name Exists Or Not Boolean Value exists And Is False\"}]; }]}}}}\"\"\"}'})}})))});\".format()))) ){ print(''.join([ f 'rstrip', lambda : {'' . join(['\\n'], (' '.split()))}\"})) }) '''), (\"{{')(\\\\{}{'))())\")))){print(\"\\xad\")({(({\\\\)}})()).next().set(\"\\\\\").getIndex(\".<index>)\")));return;}}{{(\"\" \\__|__)').insertionpoint(_tuple)},True});if(){'(('.{'['A':','+ str +']}{\\\\\"molty \\\"}).checkout'));for{}else continue},break])';coding'} else {'FINISH ME PLEASE'}, True}),fal$e]]\");     pylint --disable warnings __import__(self.__name += \"_vcs\"), Fetch(__configurations(), githubTokenVariable)])(&lt;/code><script type&#gt;\"UglifierJavascript\"/\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.9,         # slightly higher → more creative\n",
    "            top_p=0.9,               # nucleus sampling\n",
    "            do_sample=True,          # allows stochastic sampling\n",
    "            repetition_penalty=1.2,  # discourages exact phrase repetition\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Prompt ---\\n\")\n",
    "        print(prompt)\n",
    "        print(\"\\n--- Generated Output ---\\n\")\n",
    "        print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "324c8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"you are a senior software engineer, create function to add two numbers in python\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.9,         # slightly higher → more creative\n",
    "        top_p=0.9,               # nucleus sampling\n",
    "        do_sample=True,          # allows stochastic sampling\n",
    "        repetition_penalty=1.2,  # discourages exact phrase repetition\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07b412f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt ---\n",
      "\n",
      "you are a senior software engineer, create function to add two numbers in python\n",
      "\n",
      "--- Generated Output ---\n",
      "\n",
      "you are a senior software engineer, create function to add two numbers in python\n",
      "    def add(x , y):  #defining the functions\n",
      "        return x +y   // returning addition of both number inputs.     ```python# Adding Code Here ```print(\"Addition: \",add())This will not work since 'request' is undefined when run on an interactive Python environment like Jupython notebook or IDLE (it does exist inside scripts but isn’t available as partof any module)If you have code that needs input from this user at runtime then it might look something along these lines - \"Enter your name:\"name = raw_input() print('Hello %s!'%nme Hello world program by Alex Kudelski-Touheeb and Alex Blewitt also known simply referred just hello World Coding practice for beginners here we try our first coding experience with printing out messages such programming concepts can be challenging sometimes however they offer ways where one step gets easier each day”.-Chris Pattis\"\"\"     A simple way would include running through all modules within file/folder containing py files if necessary.\"\"\"elseif cnt = 0; rlwaitch = $stdin[1] || \"\" } else { echo \"#!/usr/bin/\" . $argv[-2]; while (($cpt < 5)) && (!isset(&lt;&amp;3@&gt;) ){exec & ldapsearch --getpac -LLLN | grep ^dn > dnsfile ; printf \"\\r\"; fflush();sleep=4*6*(++i); system(\"/sbin//ping\", \"-q\",\"-\"+f->hostip+$ss)\" >> /var log \"/dev\"/home/{USER}/logtest\".\"date\".txt\");}system('/root/.wine')}}finally {'echo': '', ':''}'})\") end}}}]]],['2','e']])},[[[\"\\xc9<`(\\xaa^[\\xe8E7~bO/\\xdgC,\\xfagWZ \\nbPF\\uadH]\\neX?G\\ueDhK\\ubAQ\\\\',\\'\\\"\\'.|\\';.U:\\'\\rbK\\ufSJ[\\\"]_\\'].I.,\\\":.+-.-\\'];V\\ttY.\\nuB\\uiO.'},\\'],([{'__init__(self), '\\'), ('__del__)]((__main..py...src--../subprocess-----)\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n--- Prompt ---\\n\")\n",
    "print(prompt)\n",
    "print(\"\\n--- Generated Output ---\\n\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad77fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
